{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "                 context_length, dropout,\n",
    "                 num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # 1\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # 2\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1),\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # 3\n",
    "        queries = self.W_query(x) # 3\n",
    "        values = self.W_value(x) # 3\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) # 4\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2) # 5\n",
    "        queries = queries.transpose(1, 2) # 5\n",
    "        values = values.transpose(1, 2) # 5\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3) # omega # 6\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # 7\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) # 8\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # 9\n",
    "        context_vec = context_vec.contiguous().view( # 10\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "\n",
    "        context_vec = self.out_proj(context_vec) # 11\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # layers to train the model\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (\n",
    "            1 + torch.tanh(\n",
    "                torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "                (x + 0.044715 * torch.pow(x, 3))\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        # multi-head attention\n",
    "        self.att = MultiHeadAttention(\n",
    "            # input dim\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            # output dim\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            # actual input length\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            # number of causal attention \n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            # masking rate\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            # if adding query, key, and value bias\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "\n",
    "        # Apply layers and activation function to train the model\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "        # normalization\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        \n",
    "        # masking\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 1\n",
    "\n",
    "        # assgin input as shortcut\n",
    "        shortcut = x\n",
    "\n",
    "        # normalize input\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # get context vector\n",
    "        x = self.att(x)\n",
    "\n",
    "        # dropout\n",
    "        x = self.drop_shortcut(x)\n",
    "\n",
    "        # shortcut: add input to output \n",
    "        x = x + shortcut # 2\n",
    "\n",
    "        # assgin transformed input to shortcut \n",
    "        shortcut = x # 3\n",
    "\n",
    "        # normalizing\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # apply linear layers and activation functions to input\n",
    "        x = self.ff(x)\n",
    "\n",
    "        # drop\n",
    "        x = self.drop_shortcut(x)\n",
    "\n",
    "        # shortcut: add input to output \n",
    "        x = x + shortcut # 4\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # create token embeddings\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "\n",
    "        # create positional embeddings\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "\n",
    "        # set drop out rate\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Apply transfomer block with n_layers\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        # Apply layer normalization to embedding layers\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "        # create output layers\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "\n",
    "        # create token embeddings\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        # create positional embeddings\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device = in_idx.device) # 1\n",
    "        )\n",
    "        \n",
    "        # combine token and positional embeddings\n",
    "        x = tok_embeds + pos_embeds\n",
    "\n",
    "        # drop some layers\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # apply transformer blocksbb\n",
    "        x = self.trf_blocks(x)\n",
    "\n",
    "        # normalizing\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # apply linear function to x and return probbaility of each token and text\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPT to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768, padding_idx=768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 We shorten the context length from 1,024 to 256 tokens.\n",
    "#2 It’s possible and common to set dropout to 0.\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,    #1\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1,       #2\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx,  # 1\n",
    "                         max_new_tokens, context_size):\n",
    "    \n",
    "    # iterate number of max new tokens provided\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # extract last number of context size\n",
    "        idx_cond = idx[:, -context_size:] # 2\n",
    "\n",
    "        # Disables gradient tracking since we are not training yet\n",
    "        with torch.no_grad():\n",
    "            # Obtain logits\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # only extract the last row from a tensor\n",
    "        logits = logits[:, -1, :] # 3\n",
    "\n",
    "        # Obtain probability through softmax\n",
    "        # Probability of each token in vocabulary\n",
    "        probas = torch.softmax(logits, dim = -1) # 4\n",
    "        \n",
    "        # find the max probability\n",
    "        idx_next = torch.argmax(probas, dim = -1, keepdim = True) # 5\n",
    "        \n",
    "        # find the index corresponding to the max proba\n",
    "        idx = torch.cat((idx, idx_next), dim = 1) # 6\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: \n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "    # 1\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    # 2\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "embeddings = text_to_token_ids(start_context, tokenizer)\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = embeddings,\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 Calculating the text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  targets are the inputs but shifted one position forward\n",
    "\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])  #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "#1 Disables gradient tracking since we are not training yet\n",
    "#2 Probability of each token in vocabulary\n",
    "\n",
    "with torch.no_grad():     #1\n",
    "    logits = model(inputs)\n",
    "    \n",
    "probas = torch.softmax(logits, dim=-1)     #2\n",
    "print(probas.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "#1 First batch\n",
    "#2 Second batch\n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "# The model produces random text \n",
    "# that is different from the target text because it has not been trained yet. \n",
    "\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.8852e-05, 1.5172e-05, 1.1698e-05,  ..., 2.2408e-05,\n",
       "          6.9822e-06, 1.8781e-05],\n",
       "         [9.1619e-06, 1.0067e-05, 7.8848e-06,  ..., 2.9088e-05,\n",
       "          6.0139e-06, 1.3577e-05],\n",
       "         [2.9887e-05, 8.8599e-06, 1.5754e-05,  ..., 3.5435e-05,\n",
       "          1.4104e-05, 1.3535e-05]],\n",
       "\n",
       "        [[1.2571e-05, 2.0535e-05, 1.4342e-05,  ..., 1.0396e-05,\n",
       "          3.4776e-05, 1.4245e-05],\n",
       "         [7.2785e-06, 1.7863e-05, 1.0568e-05,  ..., 2.1211e-05,\n",
       "          1.1390e-05, 1.5565e-05],\n",
       "         [2.9499e-05, 3.3594e-05, 4.1009e-05,  ..., 6.5304e-06,\n",
       "          5.8152e-05, 1.3705e-05]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4514e-05, 3.1054e-05, 1.1567e-05])\n",
      "Text 2: tensor([1.0343e-05, 5.6737e-05, 4.7620e-06])\n"
     ]
    }
   ],
   "source": [
    "# batch: 0\n",
    "# [0, 1, 2]: extract the first three rows\n",
    "# targets[0] = [a, b, c] where a, b, c are three indices corresponding to three words\n",
    "\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5045, -10.3798, -11.3674, -11.4792,  -9.7771, -12.2549])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7938)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7938)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape:  torch.Size([2, 3, 50257])\n",
      "Targets shape:  torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape: \", logits.shape)\n",
    "print(\"Targets shape: \", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits:  torch.Size([6, 50257])\n",
      "Flattened targets:  torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# combine batches\n",
    "\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits: \", logits_flat.shape)\n",
    "print(\"Flattened targets: \", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7938)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48717.6914)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding = 'utf-8') as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters:  20479\n",
      "Tokens:  5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters: \", total_characters)\n",
    "print(\"Tokens: \", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetv1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt) #1\n",
    "        \n",
    "        # The stride setting dictates the number of positions the inputs shift across batches, \n",
    "        #   emulating a sliding window approach\n",
    "        for i in range(0, len(token_ids) - max_length, stride): #2\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunck = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunck))\n",
    "    \n",
    "    def __len__(self):  #3\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):     #4\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")                         #1\n",
    "    dataset = GPTDatasetv1(txt, tokenizer, max_length, stride)   #2\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,     #3\n",
    "        num_workers=num_workers     #4\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size = 2,\n",
    "    max_length = GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last = True,\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size = 2,\n",
    "    max_length = GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last = False,\n",
    "    shuffle = False,\n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 The transfer to a gievn device allows us to transfer the data to a GPU\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)         #1\n",
    "    target_batch = target_batch.to(device)      \n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Iteratives over all batches if no fixed num_batches is specified\n",
    "#2 Reduces the number of batches to match the total number of batches in the data loader if num_batches exceeds the number of batches in the data loader\n",
    "#3 Sums loss for each batch\n",
    "#4 Averages the loss over all batches\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)     #1\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))   #2\n",
    "        \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()    #3\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches    #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\42128\\miniconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:128: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 1: invalid argument (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987384902106392\n",
      "Validation loss: 10.980905532836914\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device=device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device=device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, \n",
    "                   device, eval_iter):\n",
    "    # 1 Dropout is disabled during evaluation for stable, reproducible results.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 2 Disables gradient tracking, which is not required during evaluation, \n",
    "        #   to reduce the computational overhead\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches = eval_iter\n",
    "        )\n",
    "        \n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches = eval_iter\n",
    "        )\n",
    "    \n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model = model, idx = encoded, \n",
    "                                         max_new_tokens = 50, context_size = context_size)\n",
    "        \n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    \n",
    "    # 1 Compact print format\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, \n",
    "                       optimizer, device, num_epochs, \n",
    "                       eval_freq, eval_iter, start_context, \n",
    "                       tokenizer):\n",
    "    # 1 Initializes lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # 2 Starts the main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # 3 Resets loss gradients from the previous batch iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # calculate loss value over each batch\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "\n",
    "            # 4 Calculates loss gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # 5 Updates model weights using loss gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # numel(): returns the total number of elements in the input tensor\n",
    "            tokens_seen += input_batch.numel()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # 6 Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "\n",
    "                # evaluate model by trainning loss and validation loss\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                \n",
    "                print(f\"Ep {epoch + 1} (Step {global_step: 06d}): \"\n",
    "                      f\"Train loss {train_loss: .3f}\",\n",
    "                      f\"Val loss {val_loss: .3f}\")\n",
    "                \n",
    "        # 7 Prints a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    \n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step  00000): Train loss  9.784 Val loss  9.928\n",
      "Ep 1 (Step  00005): Train loss  7.986 Val loss  8.336\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step  00010): Train loss  6.754 Val loss  7.049\n",
      "Ep 2 (Step  00015): Train loss  6.114 Val loss  6.573\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n",
      "Ep 3 (Step  00020): Train loss  5.525 Val loss  6.489\n",
      "Ep 3 (Step  00025): Train loss  5.325 Val loss  6.389\n",
      "Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n",
      "Ep 4 (Step  00030): Train loss  4.766 Val loss  6.361\n",
      "Ep 4 (Step  00035): Train loss  4.462 Val loss  6.255\n",
      "Every effort moves you of the to the picture--as of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n",
      "Ep 5 (Step  00040): Train loss  3.835 Val loss  6.196\n",
      "Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n",
      "Ep 6 (Step  00045): Train loss  3.356 Val loss  6.140\n",
      "Ep 6 (Step  00050): Train loss  2.865 Val loss  6.112\n",
      "Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.         \"Oh, I had a, and down, and he was his\n",
      "Ep 7 (Step  00055): Train loss  2.352 Val loss  6.139\n",
      "Ep 7 (Step  00060): Train loss  2.090 Val loss  6.179\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs. \"Oh, and I was, and in an unusual degree to the house, and I had been at my elbow and as I had been the--because he had always _\n",
      "Ep 8 (Step  00065): Train loss  1.526 Val loss  6.175\n",
      "Ep 8 (Step  00070): Train loss  1.277 Val loss  6.177\n",
      "Every effort moves you?\" \"I didn't bear the picture--I told me. \"Oh, and I was, one of Jack's degree to the display of the his head to look up at the honour being _mine_--because he didn't want\n",
      "Ep 9 (Step  00075): Train loss  1.004 Val loss  6.276\n",
      "Ep 9 (Step  00080): Train loss  0.722 Val loss  6.280\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 10 (Step  00085): Train loss  0.509 Val loss  6.322\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    # 1 The .parameters() method returns all trainable weight parameters of the model\n",
    "    model.parameters(),\n",
    "    lr = 0.0004, weight_decay = 0.1\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs = num_epochs, eval_freq = 5, eval_iter = 5, \n",
    "    start_context = \"Every effort moves you\", tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    #1 Creates a second x-axis that shares the same y-axis\n",
    "    ax2 = ax1.twiny()\n",
    "\n",
    "    #2 Invisible plot for aligning ticks\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding strategies to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
