{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "                 context_length, dropout,\n",
    "                 num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # 1\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # 2\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1),\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # 3\n",
    "        queries = self.W_query(x) # 3\n",
    "        values = self.W_value(x) # 3\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) # 4\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2) # 5\n",
    "        queries = queries.transpose(1, 2) # 5\n",
    "        values = values.transpose(1, 2) # 5\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3) # omega # 6\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # 7\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) # 8\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # 9\n",
    "        context_vec = context_vec.contiguous().view( # 10\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "\n",
    "        context_vec = self.out_proj(context_vec) # 11\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # layers to train the model\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (\n",
    "            1 + torch.tanh(\n",
    "                torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "                (x + 0.044715 * torch.pow(x, 3))\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        # multi-head attention\n",
    "        self.att = MultiHeadAttention(\n",
    "            # input dim\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            # output dim\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            # actual input length\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            # number of causal attention \n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            # masking rate\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            # if adding query, key, and value bias\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "\n",
    "        # Apply layers and activation function to train the model\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "        # normalization\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        \n",
    "        # masking\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 1\n",
    "\n",
    "        # assgin input as shortcut\n",
    "        shortcut = x\n",
    "\n",
    "        # normalize input\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # get context vector\n",
    "        x = self.att(x)\n",
    "\n",
    "        # dropout\n",
    "        x = self.drop_shortcut(x)\n",
    "\n",
    "        # shortcut: add input to output \n",
    "        x = x + shortcut # 2\n",
    "\n",
    "        # assgin transformed input to shortcut \n",
    "        shortcut = x # 3\n",
    "\n",
    "        # normalizing\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # apply linear layers and activation functions to input\n",
    "        x = self.ff(x)\n",
    "\n",
    "        # drop\n",
    "        x = self.drop_shortcut(x)\n",
    "\n",
    "        # shortcut: add input to output \n",
    "        x = x + shortcut # 4\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # create token embeddings\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "\n",
    "        # create positional embeddings\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "\n",
    "        # set drop out rate\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Apply transfomer block with n_layers\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        # Apply layer normalization to embedding layers\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "        # create output layers\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "\n",
    "        # create token embeddings\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        # create positional embeddings\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device = in_idx.device) # 1\n",
    "        )\n",
    "        \n",
    "        # combine token and positional embeddings\n",
    "        x = tok_embeds + pos_embeds\n",
    "\n",
    "        # drop some layers\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # apply transformer blocksbb\n",
    "        x = self.trf_blocks(x)\n",
    "\n",
    "        # normalizing\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # apply linear function to x and return probbaility of each token and text\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPT to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768, padding_idx=768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 We shorten the context length from 1,024 to 256 tokens.\n",
    "#2 It’s possible and common to set dropout to 0.\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,    #1\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1,       #2\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx,  # 1\n",
    "                         max_new_tokens, context_size):\n",
    "    \n",
    "    # iterate number of max new tokens provided\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # extract last number of context size\n",
    "        idx_cond = idx[:, -context_size:] # 2\n",
    "\n",
    "        # Disables gradient tracking since we are not training yet\n",
    "        with torch.no_grad():\n",
    "            # Obtain logits\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # only extract the last row from a tensor\n",
    "        logits = logits[:, -1, :] # 3\n",
    "\n",
    "        # Obtain probability through softmax\n",
    "        # Probability of each token in vocabulary\n",
    "        probas = torch.softmax(logits, dim = -1) # 4\n",
    "        \n",
    "        # find the max probability\n",
    "        idx_next = torch.argmax(probas, dim = -1, keepdim = True) # 5\n",
    "        \n",
    "        # find the index corresponding to the max proba\n",
    "        idx = torch.cat((idx, idx_next), dim = 1) # 6\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: \n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "    # 1\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    # 2\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "embeddings = text_to_token_ids(start_context, tokenizer)\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = embeddings,\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 Calculating the text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  targets are the inputs but shifted one position forward\n",
    "\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])  #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "#1 Disables gradient tracking since we are not training yet\n",
    "#2 Probability of each token in vocabulary\n",
    "\n",
    "with torch.no_grad():     #1\n",
    "    logits = model(inputs)\n",
    "    \n",
    "probas = torch.softmax(logits, dim=-1)     #2\n",
    "print(probas.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "#1 First batch\n",
    "#2 Second batch\n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "# The model produces random text \n",
    "# that is different from the target text because it has not been trained yet. \n",
    "\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.8852e-05, 1.5172e-05, 1.1698e-05,  ..., 2.2408e-05,\n",
       "          6.9822e-06, 1.8781e-05],\n",
       "         [9.1619e-06, 1.0067e-05, 7.8848e-06,  ..., 2.9088e-05,\n",
       "          6.0139e-06, 1.3577e-05],\n",
       "         [2.9887e-05, 8.8599e-06, 1.5754e-05,  ..., 3.5435e-05,\n",
       "          1.4104e-05, 1.3535e-05]],\n",
       "\n",
       "        [[1.2571e-05, 2.0535e-05, 1.4342e-05,  ..., 1.0396e-05,\n",
       "          3.4776e-05, 1.4245e-05],\n",
       "         [7.2785e-06, 1.7863e-05, 1.0568e-05,  ..., 2.1211e-05,\n",
       "          1.1390e-05, 1.5565e-05],\n",
       "         [2.9499e-05, 3.3594e-05, 4.1009e-05,  ..., 6.5304e-06,\n",
       "          5.8152e-05, 1.3705e-05]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4514e-05, 3.1054e-05, 1.1567e-05])\n",
      "Text 2: tensor([1.0343e-05, 5.6737e-05, 4.7620e-06])\n"
     ]
    }
   ],
   "source": [
    "# batch: 0\n",
    "# [0, 1, 2]: extract the first three rows\n",
    "# targets[0] = [a, b, c] where a, b, c are three indices corresponding to three words\n",
    "\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5045, -10.3798, -11.3674, -11.4792,  -9.7771, -12.2549])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7938)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7938)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape:  torch.Size([2, 3, 50257])\n",
      "Targets shape:  torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape: \", logits.shape)\n",
    "print(\"Targets shape: \", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits:  torch.Size([6, 50257])\n",
      "Flattened targets:  torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# combine batches\n",
    "\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits: \", logits_flat.shape)\n",
    "print(\"Flattened targets: \", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7938)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48717.6914)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding = 'utf-8') as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters:  20479\n",
      "Tokens:  5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters: \", total_characters)\n",
    "print(\"Tokens: \", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetv1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt) #1\n",
    "        \n",
    "        # The stride setting dictates the number of positions the inputs shift across batches, \n",
    "        #   emulating a sliding window approach\n",
    "        for i in range(0, len(token_ids) - max_length, stride): #2\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunck = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunck))\n",
    "    \n",
    "    def __len__(self):  #3\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):     #4\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")                         #1\n",
    "    dataset = GPTDatasetv1(txt, tokenizer, max_length, stride)   #2\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,     #3\n",
    "        num_workers=num_workers     #4\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size = 2,\n",
    "    max_length = GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last = True,\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size = 2,\n",
    "    max_length = GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last = False,\n",
    "    shuffle = False,\n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 The transfer to a gievn device allows us to transfer the data to a GPU\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)         #1\n",
    "    target_batch = target_batch.to(device)      \n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Iteratives over all batches if no fixed num_batches is specified\n",
    "#2 Reduces the number of batches to match the total number of batches in the data loader if num_batches exceeds the number of batches in the data loader\n",
    "#3 Sums loss for each batch\n",
    "#4 Averages the loss over all batches\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)     #1\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))   #2\n",
    "        \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()    #3\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches    #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\42128\\miniconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:128: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 1: invalid argument (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987384902106392\n",
      "Validation loss: 10.980905532836914\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device=device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device=device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, \n",
    "                   device, eval_iter):\n",
    "    # 1 Dropout is disabled during evaluation for stable, reproducible results.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 2 Disables gradient tracking, which is not required during evaluation, \n",
    "        #   to reduce the computational overhead\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches = eval_iter\n",
    "        )\n",
    "        \n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches = eval_iter\n",
    "        )\n",
    "    \n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model = model, idx = encoded, \n",
    "                                         max_new_tokens = 50, context_size = context_size)\n",
    "        \n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    \n",
    "    # 1 Compact print format\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, \n",
    "                       optimizer, device, num_epochs, \n",
    "                       eval_freq, eval_iter, start_context, \n",
    "                       tokenizer):\n",
    "    # 1 Initializes lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # 2 Starts the main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # 3 Resets loss gradients from the previous batch iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # calculate loss value over each batch\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "\n",
    "            # 4 Calculates loss gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # 5 Updates model weights using loss gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # numel(): returns the total number of elements in the input tensor\n",
    "            tokens_seen += input_batch.numel()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # 6 Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "\n",
    "                # evaluate model by trainning loss and validation loss\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                \n",
    "                print(f\"Ep {epoch + 1} (Step {global_step: 06d}): \"\n",
    "                      f\"Train loss {train_loss: .3f}\",\n",
    "                      f\"Val loss {val_loss: .3f}\")\n",
    "                \n",
    "        # 7 Prints a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    \n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    # 1 The .parameters() method returns all trainable weight parameters of the model\n",
    "    model.parameters(),\n",
    "    lr = 0.0004, weight_decay = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs = num_epochs, eval_freq = 5, eval_iter = 5, \n",
    "    start_context = \"Every effort moves you\", tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    #1 Creates a second x-axis that shares the same y-axis\n",
    "    ax2 = ax1.twiny()\n",
    "\n",
    "    #2 Invisible plot for aligning ticks\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding strategies to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768, padding_idx=768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: \n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren Mortgage TT remember gard ACTIONSussedOND Land Engeleddedemate breaths proxies GalaxyForm\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens = 25,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8\n",
    "}\n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim = 0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "# To implement a probabilistic sampling process, we can now \n",
    "# replace argmax with the multinomial function in PyTorch\n",
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples = 1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    # sample a number from probas for 1000 times\n",
    "    sample = [torch.multinomial(probas, num_samples = 1).item() \n",
    "              for i in range(1000)]\n",
    "    \n",
    "    # use binominal distribution to count the unique sample\n",
    "    sample_ids = torch.bincount(torch.tensor(sample))\n",
    "    \n",
    "    for i, freq in enumerate(sample_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    # divide logits by a value greater than 0\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1, 0.1, 5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], \n",
    "                   bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 Top-K sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits:  tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions:  tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits: \", top_logits)\n",
    "print(\"Top positions: \", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits =  torch.where(\n",
    "    # 1 Identifies logits less than the minimum in the top 3\n",
    "    condition = next_token_logits < top_logits[-1],\n",
    "    # 2  Assigns –inf to these lower logits\n",
    "    input = torch.tensor(float(\"-inf\")),\n",
    "    # 3 Retains the original logits for all other tokens\n",
    "    other = next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "tok_probas = torch.softmax(new_logits, dim = 0)\n",
    "print(tok_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.3 Modifying the text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "             temperatures = 0.0, top_k = None, eos_id = None):\n",
    "    # 1 The for loop is the same as before: gets logits and only focuses on the last time step.\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        if top_k is not None:\n",
    "           # 2 Filters logits with top_k sampling\n",
    "           top_logits, _ = torch.topk(logits, top_k)\n",
    "           min_val = top_logits[:, -1]\n",
    "           logits = torch.where(\n",
    "               logits < min_val,\n",
    "               torch.tensor(float(\"-inf\")).to(logits.device),\n",
    "               logits\n",
    "           )\n",
    "        \n",
    "        if temperatures > 0.0:\n",
    "            # 3 Applies temperature scaling\n",
    "            logits = logits / temperatures\n",
    "            probas = torch.softmax(logits, dim = -1)\n",
    "            idx_next = torch.multinomial(probas, num_samples = 1)\n",
    "        else:\n",
    "            # 4 Carries out greedy next-token selection as before when temperature scaling is disabled\n",
    "            idx_next = torch.argmax(logits, dim = -1, keepdim = True)\n",
    "        \n",
    "        # 5 Stops generating early if end-of-sequence token is encountered\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim = 1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: \n",
      " Every effort moves you inferred rolleduint fabricationagos remarkably hereuced saints freewaylookOkayRand salary baseless\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens = 15,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k = 25,\n",
    "    temperatures = 1.4\n",
    ")\n",
    "\n",
    "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: \n",
      " Every effort moves you Samoa Committees *) bicycle109urban poisonous Alexandacha ENG Ornlashoof battling principally\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens = 15,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k = 10,\n",
    "    temperatures = 0.5\n",
    ")\n",
    "\n",
    "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Loading and saving model weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\42128\\AppData\\Local\\Temp\\ipykernel_27656\\2709282499.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.path\", map_location = device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768, padding_idx=768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.path\", map_location = device))\n",
    "\n",
    "# we don’t want to randomly drop out any of the information the network has learned. \n",
    "# Using model.eval() switches the model to evaluation mode for inference, \n",
    "# disabling the dropout layers of the model.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save both model and optimizer\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.path\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\42128\\AppData\\Local\\Temp\\ipykernel_30200\\1162813105.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load(\"model_and_optimizer.path\")[\"optimizer_state_dict\"])\n",
      "C:\\Users\\42128\\AppData\\Local\\Temp\\ipykernel_30200\\1162813105.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model_and_optimizer.path\")[\"model_state_dict\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.load_state_dict(torch.load(\"model_and_optimizer.path\")[\"optimizer_state_dict\"])\n",
    "model.load_state_dict(torch.load(\"model_and_optimizer.path\")[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768, padding_idx=768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x1b38c653a60>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size = \"124M\",\n",
    "    models_dir = \"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "# wte: token embedding weight\n",
    "print(params[\"wte\"])\n",
    "\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768, padding_idx=768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
    "                         \"Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):           \n",
    "    #1 Sets the model’s positional and token embedding weights to those specified in params.\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    #2 Iterates over each transformer block in the model\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "\n",
    "        #3 The np.split function is used to divide the attention and bias weights into \n",
    "        # three equal parts for the query, key, and value components.\n",
    "\n",
    "        # attention weights\n",
    "        q_w, k_w, v_w = np.split(                            \n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        # bias weights\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        # weight tensor for the output projection layer\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # weight and bias from layers\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # normalizing \n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    #4 he original GPT-2 model by OpenAI reused the token embedding weights in the output layer to \n",
    "    # reduce the total number of parameters, which is a concept known as weight tying.\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768, padding_idx=768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: \n",
      " Every effort moves you toward an equal share for each vote plus half. Inequality is often not an accurate representation of human worth; to know the\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model = gpt,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens = 25,\n",
    "    context_size = NEW_CONFIG[\"context_length\"],\n",
    "    top_k = 50,\n",
    "    temperatures = 1.5\n",
    ")\n",
    "\n",
    "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the training and validation set losses of the GPTModel with the pretrained weights from OpenAI on the “The Verdict” dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.7546131345960827\n",
      "Validation loss: 3.5596015453338623\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt.to(device)\n",
    "\n",
    "# train loader is using the verdict data\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, gpt, device=device)\n",
    "    val_loss = calc_loss_loader(val_loader, gpt, device=device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with GPT-2 models of different sizes—for example, the largest 1,558 million parameter model—and compare the generated text to the 124 million model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1600, padding_idx=1600)\n",
       "  (pos_emb): Embedding(1024, 1600)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (24): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (25): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (26): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (27): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (28): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (29): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (30): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (31): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (32): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (33): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (34): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (35): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (36): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (37): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (38): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (39): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (40): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (41): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (42): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (43): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (44): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (45): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (46): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (47): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW_CONFIG.update(model_configs[\"gpt2-xl (1558M)\"])\n",
    "gpt_large = GPTModel(NEW_CONFIG)\n",
    "gpt_large.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: \n",
      " Every effort moves you mocking timestamp Scholarship Stories marshroeOM Equ therm Lawsmb passports convince647mob overdueaningcalaughty licencesYou painting serotoninmediamb\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model = gpt_large,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens = 25,\n",
    "    context_size = NEW_CONFIG[\"context_length\"],\n",
    "    top_k = 50,\n",
    "    temperatures = 1.5\n",
    ")\n",
    "\n",
    "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
